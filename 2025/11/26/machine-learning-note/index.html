<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="基础概念1. 机器学习的两大类 监督学习（supervised learning）有标注数据（label），用于分类、回归 无监督学习（unsupervised learning）无标注数据，用于聚类、降维等   2. 基本流程：从数据到模型预测① 训练数据（Training Data） 包含多个示例（instance&#x2F;example），每个示例由一组属性（attribute&#x2F;">
<meta property="og:type" content="article">
<meta property="og:title" content="machine_learning_note">
<meta property="og:url" content="https://minjohnzi.github.io/2025/11/26/machine-learning-note/index.html">
<meta property="og:site_name" content="Just Writing Something">
<meta property="og:description" content="基础概念1. 机器学习的两大类 监督学习（supervised learning）有标注数据（label），用于分类、回归 无监督学习（unsupervised learning）无标注数据，用于聚类、降维等   2. 基本流程：从数据到模型预测① 训练数据（Training Data） 包含多个示例（instance&#x2F;example），每个示例由一组属性（attribute&#x2F;">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-11-26T13:31:59.000Z">
<meta property="article:modified_time" content="2025-11-26T14:19:41.632Z">
<meta property="article:author" content="mjmj">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>machine_learning_note</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2025/12/06/decision-tree-md/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://minjohnzi.github.io/2025/11/26/machine-learning-note/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&text=machine_learning_note"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&is_video=false&description=machine_learning_note"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=machine_learning_note&body=Check out this article: https://minjohnzi.github.io/2025/11/26/machine-learning-note/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&name=machine_learning_note&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&t=machine_learning_note"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%A4%E5%A4%A7%E7%B1%BB"><span class="toc-number">1.0.1.</span> <span class="toc-text">1. 机器学习的两大类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">2. 基本流程：从数据到模型预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%81%87%E8%AE%BE%E3%80%81%E7%9C%9F%E7%9B%B8%E4%B8%8E%E5%AD%A6%E4%B9%A0%E5%99%A8"><span class="toc-number">1.0.3.</span> <span class="toc-text">3. 假设、真相与学习器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%EF%BC%88Hypothesis-Space%EF%BC%89"><span class="toc-number">1.0.4.</span> <span class="toc-text">4. 假设空间（Hypothesis Space）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B%E5%88%86%E7%B1%BB"><span class="toc-number">1.0.5.</span> <span class="toc-text">5. 学习任务类型分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%B5%8B%E8%AF%95%E6%A0%B7%E6%9C%AC%E4%B8%8E%E6%B3%9B%E5%8C%96%EF%BC%88Generalization%EF%BC%89"><span class="toc-number">1.0.6.</span> <span class="toc-text">6. 测试样本与泛化（Generalization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD%EF%BC%88Inductive-Bias%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">归纳偏好（Inductive Bias）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">按学习方式分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.2.</span> <span class="toc-text">按任务类型分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD"><span class="toc-number">2.</span> <span class="toc-text">专业术语</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87-%E4%BC%BC%E7%84%B6%E6%A6%82%E7%8E%87-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">先验概率 似然概率 后验概率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87-Prior-Probability-P-H"><span class="toc-number">2.0.0.1.1.</span> <span class="toc-text">先验概率 (Prior Probability) - $P(H)$</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BC%BC%E7%84%B6%E6%A6%82%E7%8E%87-Likelihood-Probability-P-E-H"><span class="toc-number">2.0.0.1.2.</span> <span class="toc-text">似然概率 (Likelihood Probability) - $P(E|H)$</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87-Posterior-Probability-P-H-E"><span class="toc-number">2.0.0.1.3.</span> <span class="toc-text">后验概率 (Posterior Probability) - $P(H|E)$</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#x-theta-%E5%92%8C-theta-x"><span class="toc-number">2.0.0.2.</span> <span class="toc-text">$x|\theta$ 和$\theta|x$</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#theta-x-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%88Posterior-Probability%EF%BC%89"><span class="toc-number">2.0.0.2.1.</span> <span class="toc-text">$\theta|x$: 后验概率（Posterior Probability）</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%90%E5%8F%98%E9%87%8F"><span class="toc-number">2.0.0.3.</span> <span class="toc-text">隐变量</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">（一）机器学习基础算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.</span> <span class="toc-text">贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%EF%BC%88%E5%9F%BA%E7%A1%80%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">贝叶斯公式（基础）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%88%86%E7%B1%BB%E8%A7%84%E5%88%99"><span class="toc-number">1.2.</span> <span class="toc-text">贝叶斯分类器的分类规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F%E5%8C%96%E7%AE%80"><span class="toc-number">1.3.</span> <span class="toc-text">后验概率公式化简</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8-%EF%BC%88Naive-Bayes%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">朴素贝叶斯分类器 （Naive Bayes）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.5.</span> <span class="toc-text">贝叶斯分类器与朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%BE%E4%BE%8B"><span class="toc-number">1.6.</span> <span class="toc-text">计算后验概率举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%87%A0%E4%B8%AA%E5%85%B3%E9%94%AE%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-number">1.7.</span> <span class="toc-text">贝叶斯分类器的几个关键知识点</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%85%AC%E5%BC%8F%EF%BC%9AMAP-Maximum-A-Posteriori-Estimation-%E5%86%B3%E7%AD%96"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">1. 公式：MAP (Maximum A Posteriori Estimation) 决策</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%B1%95%E5%BC%80"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">2. 后验概率展开</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%81%87%E8%AE%BE"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">3. 朴素贝叶斯假设</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%B9%B3%E6%BB%91"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">4. 平滑</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q-Learning"><span class="toc-number"></span> <span class="toc-text">Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#K-means-%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">K-means 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means-%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.1.</span> <span class="toc-text">K-means 算法步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VC%E7%BB%B4"><span class="toc-number">2.</span> <span class="toc-text">VC维</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#VC-%E7%BB%B4%E7%9A%84%E6%AD%A3%E5%BC%8F%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">VC 维的正式定义：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B%E7%9A%84-VC-%E7%BB%B4"><span class="toc-number">2.0.0.2.</span> <span class="toc-text">常见模型的 VC 维</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#VC-%E7%BB%B4%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.0.0.3.</span> <span class="toc-text">VC 维与泛化误差</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA-%EF%BC%88Principal-Component-Analysis-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">PCA （Principal Component Analysis) 主成分分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning-1"><span class="toc-number">4.</span> <span class="toc-text">Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number"></span> <span class="toc-text">（二）统计学习分类器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number"></span> <span class="toc-text">（三）线性模型与神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-%EF%BC%88Convolutional-Neural-Networks%EF%BC%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">CNN （Convolutional Neural Networks）卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN-%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">CNN 架构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E5%9B%9B%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number"></span> <span class="toc-text">（四）深度学习</span></a>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        machine_learning_note
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">mjmj</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-11-26T13:31:59.000Z" class="dt-published" itemprop="datePublished">2025-11-26</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><h4 id="1-机器学习的两大类"><a href="#1-机器学习的两大类" class="headerlink" title="1. 机器学习的两大类"></a>1. 机器学习的两大类</h4><ul>
<li><strong>监督学习（supervised learning）</strong><br>有标注数据（label），用于分类、回归</li>
<li><strong>无监督学习（unsupervised learning）</strong><br>无标注数据，用于聚类、降维等</li>
</ul>
<hr>
<h4 id="2-基本流程：从数据到模型预测"><a href="#2-基本流程：从数据到模型预测" class="headerlink" title="2. 基本流程：从数据到模型预测"></a>2. 基本流程：从数据到模型预测</h4><p><strong>① 训练数据（Training Data）</strong></p>
<p>包含多个示例（instance&#x2F;example），每个示例由一组属性（attribute&#x2F;feature）构成，如：</p>
<table>
<thead>
<tr>
<th>色泽</th>
<th>根蒂</th>
<th>敲声</th>
<th>好瓜</th>
</tr>
</thead>
<tbody><tr>
<td>青绿</td>
<td>蜷缩</td>
<td>浊响</td>
<td>是</td>
</tr>
<tr>
<td>乌黑</td>
<td>蜷缩</td>
<td>浊响</td>
<td>是</td>
</tr>
<tr>
<td>青绿</td>
<td>硬挺</td>
<td>清脆</td>
<td>否</td>
</tr>
<tr>
<td>乌黑</td>
<td>稍蜷</td>
<td>沉闷</td>
<td>否</td>
</tr>
</tbody></table>
<ul>
<li><p><strong>属性值</strong>：每一列的取值</p>
</li>
<li><p><strong>属性空间</strong>：所有属性及其组合的空间</p>
</li>
<li><p><strong>样本空间（输入空间）</strong>：所有可能的样本向量</p>
</li>
<li><p><strong>特征向量（feature vector）</strong>：如<br>$ x &#x3D; (\text{色泽}, \text{根蒂}, \text{敲声})<br>$</p>
</li>
<li><p><strong>标记（label）</strong>：如“好瓜是否”</p>
</li>
<li><p><strong>标记空间（输出空间）</strong></p>
</li>
</ul>
<p><strong>② 使用学习算法（learning algorithm）训练模型</strong></p>
<p>常见模型：</p>
<ul>
<li>决策树（Decision Tree）</li>
<li>神经网络（Neural Network）</li>
<li>支持向量机（SVM）</li>
<li>Boosting &#x2F; Bagging 集成方法</li>
<li>贝叶斯网（Bayesian Network）</li>
</ul>
<p>模型本质上对应一个 <strong>假设（hypothesis）</strong>。</p>
<p><strong>③ 对新样本进行预测（testing）</strong></p>
<p>示例：</p>
<ul>
<li><p>输入测试样本：<br>$<br> x &#x3D; (\text{浅白},\ \text{蜷缩},\ \text{浊响},\ ?)<br>$</p>
</li>
<li><p>模型输出预测类别：<br>$<br> \hat{y} &#x3D; \text{“是”}<br>$</p>
</li>
</ul>
<hr>
<h4 id="3-假设、真相与学习器"><a href="#3-假设、真相与学习器" class="headerlink" title="3. 假设、真相与学习器"></a>3. 假设、真相与学习器</h4><ul>
<li><strong>假设（hypothesis）</strong>：模型对输入到输出的映射</li>
<li><strong>真相（ground-truth）</strong>：真实的、理想的规律</li>
<li><strong>学习器（learner）</strong>：从训练数据中学习出假设的主体（即算法或模型）</li>
</ul>
<hr>
<h4 id="4-假设空间（Hypothesis-Space）"><a href="#4-假设空间（Hypothesis-Space）" class="headerlink" title="4. 假设空间（Hypothesis Space）"></a>4. 假设空间（Hypothesis Space）</h4><ul>
<li>模型能够表示的所有可能假设的集合</li>
<li>例如：<br>一棵决策树的不同结构、不同划分方式形成其 <strong>假设空间</strong></li>
</ul>
<p>对某个决策树例子，假设空间包含：</p>
<ul>
<li>仅使用色泽划分</li>
<li>使用色泽 → 根蒂</li>
<li>使用色泽 → 敲声</li>
<li>使用根蒂 → 敲声</li>
<li>……（各种组合）</li>
</ul>
<hr>
<h4 id="5-学习任务类型分类"><a href="#5-学习任务类型分类" class="headerlink" title="5. 学习任务类型分类"></a>5. 学习任务类型分类</h4><ul>
<li><strong>分类（classification）</strong><ul>
<li>二分类（如：是否为好瓜）</li>
<li>多分类（如 10 种数字）</li>
<li>正类 &#x2F; 反类概念</li>
</ul>
</li>
<li><strong>回归（regression）</strong><br>输出连续值，如房价预测</li>
</ul>
<h4 id="6-测试样本与泛化（Generalization）"><a href="#6-测试样本与泛化（Generalization）" class="headerlink" title="6. 测试样本与泛化（Generalization）"></a>6. 测试样本与泛化（Generalization）</h4><p> <strong>未见样本（unseen instance）</strong></p>
<p>​	模型训练时未出现的新数据。</p>
<p> <strong>未知数据分布</strong></p>
<p>​	训练数据来自某个未知的概率分布 ( P(X, Y) )。</p>
<p> <strong>独立同分布（i.i.d.）假设</strong></p>
<p>​	通常学界假设训练样本满足：<br>$(x_i, y_i) \sim P(X, Y) \quad \text{相互独立}$</p>
<p> <strong>泛化（generalization）</strong></p>
<p>​	模型对未见样本仍能保持较好预测能力。</p>
<h3 id="归纳偏好（Inductive-Bias）"><a href="#归纳偏好（Inductive-Bias）" class="headerlink" title="归纳偏好（Inductive Bias）"></a>归纳偏好（Inductive Bias）</h3><p><strong>归纳偏好（Inductive Bias）</strong>：</p>
<blockquote>
<p>机器学习算法在学习过程中对“哪一类假设”更可能成立所做的偏好性假设。</p>
</blockquote>
<p>也就是说，算法并不是在完全无偏无假设地从数据中得出结论，而是带着某种“偏好”来选择假设。</p>
<h3 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h3><h4 id="按学习方式分类"><a href="#按学习方式分类" class="headerlink" title="按学习方式分类"></a>按学习方式分类</h4><p><strong>1.1 监督学习（Supervised Learning）</strong></p>
<p>数据：有输入 (x) 和标记 (y)。<br> 任务：从训练数据学习一个从 (x \to y) 的映射。</p>
<p>常见任务：</p>
<ul>
<li><strong>分类（Classification）</strong></li>
<li><strong>回归（Regression）</strong></li>
</ul>
<p>示例算法：</p>
<ul>
<li>决策树、随机森林</li>
<li>SVM</li>
<li>逻辑回归</li>
<li>神经网络</li>
<li>KNN</li>
</ul>
<p><strong>1.2 无监督学习（Unsupervised Learning）</strong></p>
<p>数据：只有 (x)，没有标签。<br> 目标：发现数据内部结构。</p>
<p>常见任务：</p>
<ul>
<li>聚类（Clustering）：K-means、层次聚类</li>
<li>降维（Dimensionality Reduction）：PCA、t-SNE</li>
<li>密度估计、异常检测</li>
</ul>
<p><strong>1.3 半监督学习（Semi-supervised Learning）</strong></p>
<p>数据：少量标记 + 大量未标记。<br> 任务：提高模型精度，降低标注成本。</p>
<p>典型方法：</p>
<ul>
<li>自训练（Self-training）</li>
<li>图半监督（Graph-based）</li>
<li>伪标签（Pseudo-label）</li>
</ul>
<h4 id="按任务类型分类"><a href="#按任务类型分类" class="headerlink" title="按任务类型分类"></a>按任务类型分类</h4><p><strong>2.1 分类（Classification）</strong></p>
<p>输出为离散类别 (y \in {1,2,…,k})。</p>
<ul>
<li>二分类（binary classification）</li>
<li>多分类（multi-class）</li>
<li>多标签分类（multi-label）</li>
</ul>
<p>常见模型：逻辑回归、决策树、SVM、神经网络。</p>
<p><strong>2.2 回归（Regression）</strong></p>
<p>输出为连续值，如房价预测。</p>
<p>常见模型：线性回归、岭回归、SVR、神经网络。</p>
<p><strong>2.3 聚类（Clustering）</strong></p>
<p>无监督学习的典型任务。</p>
<p>算法：K-means、DBSCAN、GMM。</p>
<p><strong>2.4 降维（Dimensionality Reduction）</strong></p>
<p>目标：压缩数据维度。</p>
<p>算法：PCA、LDA、AutoEncoder。</p>
<h2 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h2><h5 id="先验概率-似然概率-后验概率"><a href="#先验概率-似然概率-后验概率" class="headerlink" title="先验概率 似然概率 后验概率"></a><strong>先验概率 似然概率 后验概率</strong></h5><p><strong>贝叶斯定理</strong>：</p>
<p>$P(H|E) &#x3D; \frac{P(E|H) \cdot P(H)}{P(E)}$</p>
<p>其中：</p>
<ul>
<li><strong>$P(H|E)$</strong> 是<strong>后验概率</strong></li>
<li><strong>$P(H)$</strong> 是<strong>先验概率</strong></li>
<li><strong>$P(E|H)$</strong> 是<strong>似然概率</strong></li>
<li><strong>$P(E)$</strong> 是<strong>证据的边缘概率</strong>（通常用于归一化，使其所有后验概率之和为 1）</li>
</ul>
<h6 id="先验概率-Prior-Probability-P-H"><a href="#先验概率-Prior-Probability-P-H" class="headerlink" title="先验概率 (Prior Probability) - $P(H)$"></a><strong>先验概率 (Prior Probability) - $P(H)$</strong></h6><p>定义</p>
<ul>
<li><strong>先验概率</strong>是指在<strong>观测到任何新数据（证据 $E$）之前</strong>，我们对某个假设或参数 $H$ 的信任程度（概率）。</li>
<li>它来源于我们已有的<strong>背景知识、历史数据、经验</strong>或<strong>主观判断</strong>。</li>
<li>它是在进行实验或收集信息<strong>之前</strong>就确定的。</li>
</ul>
<p>假设一家医院的医生想知道一名患者是否患有某种罕见疾病 $H$。</p>
<ul>
<li><strong>先验概率 $P(H)$：</strong> 根据该疾病的历史发病率，<strong>在查看该患者的任何测试结果之前</strong>，医生估计普通人群中患该疾病的概率是 <strong>$1%$</strong>。</li>
</ul>
<h6 id="似然概率-Likelihood-Probability-P-E-H"><a href="#似然概率-Likelihood-Probability-P-E-H" class="headerlink" title="似然概率 (Likelihood Probability) - $P(E|H)$"></a><strong>似然概率 (Likelihood Probability) - $P(E|H)$</strong></h6><p>定义</p>
<ul>
<li><strong>似然概率</strong>是指在<strong>假设 $H$ 成立的条件下</strong>，我们观测到当前数据（证据 $E$）的概率。</li>
<li>它描述了我们<strong>当前的数据</strong>对不同假设的支持程度。</li>
<li>请注意：它<strong>不是</strong>关于假设 $H$ 的概率，而是<strong>关于数据的概率</strong>。</li>
</ul>
<p>该患者进行了疾病检测 $E$。</p>
<ul>
<li><strong>似然概率 $P(E|H)$：</strong> 已知如果患者<strong>确实患有疾病 $H$</strong>，测试结果呈阳性（证据 $E$）的概率（即测试的灵敏度）是 <strong>$95%$</strong>。</li>
<li>这告诉我们：在“患病”这个假设下，我们看到“阳性结果”的可能性有多大。</li>
</ul>
<blockquote>
<p><strong>另一个重要的似然项</strong>是：如果患者<strong>没有患病 $H’$</strong>，测试结果仍呈阳性（证据 $E$）的概率（即假阳性率），例如 <strong>$10%$</strong>。</p>
</blockquote>
<ul>
<li>$P(E|H’)$ &#x3D; $10%$</li>
</ul>
<h6 id="后验概率-Posterior-Probability-P-H-E"><a href="#后验概率-Posterior-Probability-P-H-E" class="headerlink" title="后验概率 (Posterior Probability) - $P(H|E)$"></a>后验概率 (Posterior Probability) - $P(H|E)$</h6><p>定义</p>
<ul>
<li><p><strong>后验概率</strong>是指在<strong>观测到新数据（证据 $E$）之后</strong>，对某个假设 $H$ 的修正后的信任程度（概率）。</p>
</li>
<li><p>它是<strong>先验概率</strong>与<strong>似然概率</strong>结合的结果。</p>
</li>
<li><p>它是我们最关心的量，代表着我们<strong>更新后的知识</strong>。</p>
</li>
<li><p><strong>后验概率 $P(H|E)$：</strong> 在得知该患者的测试结果<strong>呈阳性 $E$ 之后</strong>，医生重新计算该患者<strong>患有疾病 $H$ 的概率</strong>。</p>
</li>
</ul>
<p>将上面例子中的数值代入贝叶斯定理：</p>
<ul>
<li>$P(H) &#x3D; 0.01$ (患病先验)</li>
<li>$P(E|H) &#x3D; 0.95$ (真阳性率&#x2F;似然)</li>
<li>$P(H’) &#x3D; 1 - P(H) &#x3D; 0.99$ (未患病先验)</li>
<li>$P(E|H’) &#x3D; 0.10$ (假阳性率)</li>
</ul>
<p>证据 $P(E)$ 的概率（测试呈阳性的总概率）为：</p>
<p>$P(E) &#x3D; P(E|H)P(H) + P(E|H’)P(H’)$</p>
<p>$P(E) &#x3D; (0.95 \cdot 0.01) + (0.10 \cdot 0.99) &#x3D; 0.0095 + 0.099 &#x3D; 0.1085$</p>
<p>后验概率 $P(H|E)$（在阳性结果下患病的概率）为：</p>
<p>$P(H|E) &#x3D; \frac{P(E|H) \cdot P(H)}{P(E)} &#x3D; \frac{0.95 \cdot 0.01}{0.1085} \approx 0.0875$</p>
<p><strong>总结：</strong></p>
<table>
<thead>
<tr>
<th><strong>概念</strong></th>
<th><strong>符号</strong></th>
<th><strong>描述</strong></th>
<th><strong>时间点</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>先验概率</strong></td>
<td>$P(H)$</td>
<td>对假设 $H$ 的<strong>初始信任</strong>。</td>
<td><strong>观测数据之前</strong></td>
<td>提供了<strong>背景知识</strong>。</td>
</tr>
<tr>
<td><strong>似然概率</strong></td>
<td>$P(E</td>
<td>H)$</td>
<td>观测到数据 $E$ 的<strong>可能性</strong></td>
<td><strong>观测数据时</strong></td>
</tr>
<tr>
<td><strong>后验概率</strong></td>
<td>$P(H</td>
<td>E)$</td>
<td>对假设 $H$ 的<strong>修正信任</strong>。</td>
<td><strong>观测数据之后</strong></td>
</tr>
</tbody></table>
<h5 id="x-theta-和-theta-x"><a href="#x-theta-和-theta-x" class="headerlink" title="$x|\theta$ 和$\theta|x$"></a><strong>$x|\theta$ 和$\theta|x$</strong></h5><p><strong>$x|\theta$: 似然（Likelihood）或概率分布（Probability Distribution）</strong></p>
<p><strong>定义与含义</strong></p>
<ul>
<li><strong>读作：</strong> “给定 $\theta$ 时的 $x$ 的概率” 或 “$x$ <strong>以</strong> $\theta$ 为条件的概率”。</li>
<li><strong>数学表达式：</strong> $P(x|\theta)$ 或 $f(x|\theta)$。</li>
<li><strong>角色：</strong> 它描述的是在<strong>模型参数 $\theta$ 已经确定</strong>的情况下，<strong>观测到数据 $x$ 的可能性</strong>。</li>
<li><strong>重点：</strong> 在这个表达式中，$\theta$ 是一个<strong>已知的（或假设已知的）常量</strong>，而 $x$ 是一个<strong>随机变量</strong>。</li>
</ul>
<h6 id="theta-x-后验概率（Posterior-Probability）"><a href="#theta-x-后验概率（Posterior-Probability）" class="headerlink" title="$\theta|x$: 后验概率（Posterior Probability）"></a><strong>$\theta|x$: 后验概率（Posterior Probability）</strong></h6><p><strong>定义与含义</strong></p>
<ul>
<li><strong>读作：</strong> “给定 $x$ 时的 $\theta$ 的概率” 或 “$\theta$ <strong>以</strong> $x$ 为条件的概率”。</li>
<li><strong>数学表达式：</strong> $P(\theta|x)$ 或 $f(\theta|x)$。</li>
<li><strong>角色：</strong> 它描述的是在<strong>已经观测到数据 $x$</strong> 的情况下，<strong>模型参数 $\theta$ 的概率分布</strong>。</li>
<li><strong>重点：</strong> 在这个表达式中，$x$ 是一个<strong>已知的（观测到的）常量</strong>，而 $\theta$ 被视为一个<strong>随机变量</strong>（因为它在观察到数据 $x$ 之前是不确定的）。</li>
</ul>
<ul>
<li>计算方法： $\theta|x$ 是通过贝叶斯定理计算得到的：</li>
</ul>
<p>​	$P(\theta|x) &#x3D; \frac{P(x|\theta) P(\theta)}{P(x)}$</p>
<p>其中：</p>
<ul>
<li>$P(\theta|x)$：<strong>后验概率</strong>（Posterior）— 我们想要计算的结果。</li>
<li>$P(x|\theta)$：<strong>似然</strong>（Likelihood）— 即上面的 $x|\theta$。</li>
<li>$P(\theta)$：<strong>先验概率</strong>（Prior）— 在观察数据 $x$ 之前，我们对 $\theta$ 的初始信念。</li>
<li>$P(x)$：<strong>边缘似然</strong>（Marginal Likelihood）— 一个归一化常数。</li>
</ul>
<p>简单来说：</p>
<ul>
<li><strong>$x|\theta$</strong>：<strong>原因</strong>（$\theta$）<strong>导致结果</strong>（$x$）的<strong>概率</strong>。</li>
<li><strong>$\theta|x$</strong>：根据<strong>结果</strong>（$x$）<strong>推断原因</strong>（$\theta$）的<strong>概率</strong>。</li>
</ul>
<h5 id="隐变量"><a href="#隐变量" class="headerlink" title="隐变量"></a>隐变量</h5><hr>
<h1 id="（一）机器学习基础算法"><a href="#（一）机器学习基础算法" class="headerlink" title="（一）机器学习基础算法"></a>（一）机器学习基础算法</h1><h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p><strong>贝叶斯分类器（Bayesian Classifier）是一类基于贝叶斯公式进行分类的模型。</strong></p>
<p>核心思想：</p>
<blockquote>
<p><strong>选择使后验概率最大的类别。</strong></p>
</blockquote>
<p>$y^*&#x3D;\arg\max\limits_{c} P(c \mid x</p>
<p>这叫 <strong>MAP（最大后验概率）分类准则</strong>。</p>
<p><strong>为什么需要贝叶斯分类器？</strong></p>
<p>分类的本质任务是：</p>
<blockquote>
<p>给定一个样本 $x$，判断它属于哪个类别 $c$。</p>
</blockquote>
<p>例如：</p>
<ul>
<li>邮件（垃圾 &#x2F; 非垃圾）</li>
<li>图像（猫 &#x2F; 狗）</li>
<li>疾病检测（阳性 &#x2F; 阴性）</li>
</ul>
<p>那么我们需要一个规则，把输入 $x$ 映射到类别 $c$。</p>
<p>贝叶斯分类器提供了一个 <strong>数学上最优的决策规则</strong>：</p>
<h3 id="贝叶斯公式（基础）"><a href="#贝叶斯公式（基础）" class="headerlink" title="贝叶斯公式（基础）"></a>贝叶斯公式（基础）</h3><p>贝叶斯分类器建立于贝叶斯公式：<br>$P(c|x)&#x3D;\frac{P(x|c)P(c)}{P(x)}$<br>其中：</p>
<ul>
<li>$P(c)$：先验概率</li>
<li>$P(x|c)$：似然（给定类别时观测到 x 的概率）</li>
<li>$P(c|x)$：后验概率（看到 x 后，它属于类 c 的概率）</li>
<li>$P(x)$：证据（统一归一化因子）</li>
</ul>
<h3 id="贝叶斯分类器的分类规则"><a href="#贝叶斯分类器的分类规则" class="headerlink" title="贝叶斯分类器的分类规则"></a>贝叶斯分类器的分类规则</h3><p>贝叶斯分类器直接选择后验概率最大的类别：<br>$h(x)&#x3D;\arg\max_{c} P(c|x)$<br>这是 <strong>理论上最优、错误率最低的分类器</strong>。</p>
<p>为什么这样选是最优？<br> 因为统计决策论的证明告诉我们：</p>
<blockquote>
<p>想最小化分类错误率，就应该选后验概率最大的类别（MAP 决策）。</p>
</blockquote>
<h3 id="后验概率公式化简"><a href="#后验概率公式化简" class="headerlink" title="后验概率公式化简"></a>后验概率公式化简</h3><p>在分类任务中，证据 $P(x)$ 对所有类别相同，所以比较：<br>$<br>P(c|x)\propto P(x|c)P(c)<br>$<br>于是分类规则写成：<br>$h(x)&#x3D;\arg\max_{c} P(x|c) P(c)$<br>分类时不必计算 $P(x)$，只需计算：$先验 × 似然$。</p>
<h3 id="朴素贝叶斯分类器-（Naive-Bayes）"><a href="#朴素贝叶斯分类器-（Naive-Bayes）" class="headerlink" title="朴素贝叶斯分类器 （Naive Bayes）"></a>朴素贝叶斯分类器 （Naive Bayes）</h3><p>真正的贝叶斯分类器要求我们知道 <strong>完整的条件概率 $P(x|c)$</strong>，但现实中：</p>
<ul>
<li>特征多（维度高）</li>
<li>样本有限</li>
<li>很难估计完整联合概率</li>
</ul>
<p>例如 100 维特征：<br> 需要估计 $P(x_1,x_2,…,x_{100}|c)$，几乎不可能。</p>
<p>于是使用<strong>特征条件独立假设</strong>：<br>$P(x|c)&#x3D;\prod_{i&#x3D;1}^d P(x_i|c)$<br>于是后验概率可以表示为：<br>$P(c|x)\propto P(c)\prod_{i&#x3D;1}^d P(x_i|c)$</p>
<ul>
<li>$d$: 特征维度</li>
</ul>
<p>这就是 <strong>朴素贝叶斯分类器</strong>。</p>
<h3 id="贝叶斯分类器与朴素贝叶斯分类器"><a href="#贝叶斯分类器与朴素贝叶斯分类器" class="headerlink" title="贝叶斯分类器与朴素贝叶斯分类器"></a>贝叶斯分类器与朴素贝叶斯分类器</h3><table>
<thead>
<tr>
<th>类型</th>
<th>定义</th>
<th>是否最优？</th>
</tr>
</thead>
<tbody><tr>
<td><strong>贝叶斯最优分类器</strong></td>
<td>完全使用真实的概率分布</td>
<td>✔ 最低错误率</td>
</tr>
<tr>
<td><strong>朴素贝叶斯分类器</strong></td>
<td>假设特征独立，近似贝叶斯最优</td>
<td>❌ 近似最优</td>
</tr>
</tbody></table>
<p>朴素贝叶斯是可实现的近似，<br>贝叶斯最优分类器是理论上的“理想状态”。</p>
<h3 id="计算后验概率举例"><a href="#计算后验概率举例" class="headerlink" title="计算后验概率举例"></a>计算后验概率举例</h3><p>要分类水果是“苹果”或“香蕉”：</p>
<p>假设：</p>
<ul>
<li>$P(\text{苹果})&#x3D;0.4$</li>
<li>$P(\text{香蕉})&#x3D;0.6$</li>
</ul>
<p>观察到特征 $x&#x3D;$“长且黄”：</p>
<p>假设似然：</p>
<ul>
<li>$P(x|\text{苹果})&#x3D;0.1$</li>
<li>$P(x|\text{香蕉})&#x3D;0.5$</li>
</ul>
<p>计算：<br>$P(\text{苹果}|x)\propto 0.4 \times 0.1 &#x3D; 0.04$<br>比较大小即可得出：</p>
<h3 id="贝叶斯分类器的几个关键知识点"><a href="#贝叶斯分类器的几个关键知识点" class="headerlink" title="贝叶斯分类器的几个关键知识点"></a>贝叶斯分类器的几个关键知识点</h3><h5 id="1-公式：MAP-Maximum-A-Posteriori-Estimation-决策"><a href="#1-公式：MAP-Maximum-A-Posteriori-Estimation-决策" class="headerlink" title="1. 公式：MAP (Maximum A Posteriori Estimation) 决策"></a>1. 公式：MAP (Maximum A Posteriori Estimation) 决策</h5><p>$<br>h(x)&#x3D;\arg\max_c P(c|x)<br>$</p>
<h5 id="2-后验概率展开"><a href="#2-后验概率展开" class="headerlink" title="2. 后验概率展开"></a>2. 后验概率展开</h5><p>$<br>P(c|x)\propto P(x|c)P(c)<br>$</p>
<h5 id="3-朴素贝叶斯假设"><a href="#3-朴素贝叶斯假设" class="headerlink" title="3. 朴素贝叶斯假设"></a>3. 朴素贝叶斯假设</h5><p>$P(x|c)&#x3D;\prod_i P(x_i|c)$</p>
<h5 id="4-平滑"><a href="#4-平滑" class="headerlink" title="4. 平滑"></a>4. 平滑</h5><p>避免概率为 0，要用 <strong>拉普拉斯平滑</strong>：<br>$P(x_i|c)&#x3D;\frac{N_{i,c}+1}{N_c+|V|}$</p>
<blockquote>
<p>由于如果样本中的某个特征从未出现，那么假设 $P(x|c)&#x3D;\prod_i P(x_i|c)$</p>
<p>中，一旦某项为0，就会导致整体变0。</p>
<p>这会导致</p>
<ul>
<li><p>整个类别的可能性被归零</p>
</li>
<li><p>完全误判</p>
</li>
<li><p>分类器崩溃</p>
</li>
</ul>
<p>所以需要使用<strong>拉普拉斯平滑（Add-One）</strong> <strong>$P(x_i|c)&#x3D;\frac{N_{i,c}+1}{N_c+|V|}$</strong></p>
<ul>
<li><p><strong>$N_{i,c}+1$</strong>：在计数的基础上<strong>加1</strong> $ →$ 即使出现次数为 0，仍然给它一个最小概率</p>
</li>
<li><p><strong>$|V|$</strong>：可能的取值总数（比如词典大小、离散特征空间大小）</p>
</li>
<li><p><strong>$N_c+|V|$</strong>：保持概率归一化（所有概率之和为 1）</p>
</li>
</ul>
</blockquote>
<hr>
<h1 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h1><hr>
<h2 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means 算法"></a>K-means 算法</h2><p><strong>K-means</strong> 是一种经典的 <strong>基于原型（prototype-based）</strong> 的 <strong>无监督聚类算法</strong>，通过最小化样本到聚类中心的平方距离完成聚类。</p>
<p>目标：<br> 将数据集划分为 $K$ 个簇，每个簇由一个“均值向量（质心）”代表。</p>
<p><strong>目标函数（优化目标）</strong></p>
<p>K-means 的目标是最小化簇内误差平方和（Sum of Squared Errors，SSE）：<br>$<br>J&#x3D;i&#x3D;1∑Kx∈Ci∑∥x−μi∥2<br>$<br>其中：</p>
<ul>
<li>$C_i$：第 $i$ 个簇</li>
<li>$\mu_i$：第 $i$ 个簇的均值中心</li>
<li>$|x - \mu_i|^2$：样本与中心的欧氏距离平方</li>
</ul>
<p>即：</p>
<blockquote>
<p><strong>每个样本靠近某个中心 → 整体偏差最小。</strong></p>
</blockquote>
<h3 id="K-means-算法步骤"><a href="#K-means-算法步骤" class="headerlink" title="K-means 算法步骤"></a>K-means 算法步骤</h3><p><strong>Step 1：初始化中心点</strong></p>
<p>随机选择 $K$ 个样本作为初始聚类中心。<br>$<br>\mu_1^{(0)}, \mu_2^{(0)}, \ldots, \mu_K^{(0)}<br>$</p>
<p><strong>Step 2：样本分配（Assignment step）</strong></p>
<p>对每个样本 $x$，分配到最近的中心：<br>$<br>c(x) &#x3D; \arg\min_{i} |x - \mu_i|^2<br>$<br>即：</p>
<blockquote>
<p>每个点“投票”给离它最近的中心。</p>
</blockquote>
<p><strong>Step 3：更新中心（Update step）</strong></p>
<p>对每个簇 $C_i$，用簇内所有点的均值更新中心：<br>$<br>\mu_i &#x3D; \frac{1}{|C_i|} \sum_{x \in C_i} x<br>$</p>
<p><strong>Step 4：重复 Step 2-3 直到收敛</strong></p>
<p>停止条件：</p>
<ul>
<li>中心不再变化</li>
<li>或最大迭代次数达到</li>
<li>或目标函数下降不足阈值</li>
</ul>
<p>![K-means 工作流程](.&#x2F;assets&#x2F;machine-learning-note&#x2F;K-means 工作流程.png)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">  A[初始化：选择 K 个初始质心] --&gt; B[样本分配（Assignment）\n每个样本分配到最近质心]</span><br><span class="line">  B --&gt; C[更新质心（Update）\n计算各簇均值作为新质心]</span><br><span class="line">  C --&gt; D&#123;收敛？&#125;</span><br><span class="line">  D -- 否 --&gt; B</span><br><span class="line">  D -- 是 --&gt; E[输出簇标签与质心（结束）]</span><br><span class="line"></span><br><span class="line">  %% 辅助节点：参数与停止条件</span><br><span class="line">  subgraph PARAMS [参数与终止条件]</span><br><span class="line">    P1[设置 K]</span><br><span class="line">    P2[最大迭代次数 / 阈值]</span><br><span class="line">    P3[K-means++ 初始化（可选）]</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  P1 --&gt; A</span><br><span class="line">  P2 --&gt; D</span><br><span class="line">  P3 --&gt; A</span><br><span class="line"></span><br><span class="line">  %% 可点击示意图（使用你上传的本地文件路径作为 URL）</span><br><span class="line">  IMG[查看示意图（点击打开）]</span><br><span class="line">  IMG -.-&gt; A</span><br><span class="line">  click IMG &quot;/mnt/data/A_2D_digital_diagram_illustrates_the_K-means_clust.png&quot; &quot;打开 K-means 示意图&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h2 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h2><p><strong>VC 维（VC Dimension）</strong> 用于衡量一个假设空间（hypothesis space）的“容量&#x2F;复杂度”。</p>
<blockquote>
<p>VC 维 &#x3D; 假设空间能够打散（shatter）的最大样本点数。</p>
</blockquote>
<p><strong>打散（shatter)</strong>: 对于样本集合中所有可能的 $2^m$中标记方法，假设空间中都能找到某个假设完全正确的分类。</p>
<p>例如：</p>
<ul>
<li><p>若 3 个点的所有 $2^3&#x3D;8$ 中标记方式，都能被某模型正确分类 -&gt; 模型可以打散这三个点。</p>
</li>
<li><p>二维线性模型 ：</p>
<p>对于二维线性分类器（用一条直线分类），任意 3 个不共线的点： <strong>不论怎么标记</strong>都能用一条直线把正类 (+) 和负类 (–) 分开。因此二维线性分类器的 VC 维 ≥ 3。但对于 4 个点（例如凸四边形标正负交替），<strong>无法画一条直线分开</strong>，所以二维线性分类器 <strong>VC &#x3D; 3</strong>。</p>
</li>
</ul>
<h5 id="VC-维的正式定义："><a href="#VC-维的正式定义：" class="headerlink" title="VC 维的正式定义："></a><strong>VC 维的正式定义</strong>：</h5><p>设假设类为 $H$。<br> 若存在 $m$ 个样本点，使得它们被 $H$ <strong>完全打散</strong>，那么：<br>$<br>VC(H) \ge m<br>$<br>若不存在任何 $m+1$ 个点能被打散，则：<br>$<br>VC(H) &#x3D; m<br>$</p>
<h5 id="常见模型的-VC-维"><a href="#常见模型的-VC-维" class="headerlink" title="常见模型的 VC 维"></a><strong>常见模型的 VC 维</strong></h5><table>
<thead>
<tr>
<th>模型</th>
<th>VC 维</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>单个阈值分类器（1D）</td>
<td>1</td>
<td>一条数轴上的阈值</td>
</tr>
<tr>
<td>二维感知机（线性分类器）</td>
<td>3</td>
<td>可打散任意三点</td>
</tr>
<tr>
<td>d 维感知机</td>
<td>(d+1)</td>
<td>超平面</td>
</tr>
<tr>
<td>间隔为 ρ 的线性分类器</td>
<td>与间隔相关</td>
<td>间隔越大 VC 维越小</td>
</tr>
<tr>
<td>k-邻近算法（kNN）</td>
<td>无限大</td>
<td>高容量模型</td>
</tr>
</tbody></table>
<h5 id="VC-维与泛化误差"><a href="#VC-维与泛化误差" class="headerlink" title="VC 维与泛化误差"></a><strong>VC 维与泛化误差</strong></h5><p>VC 理论给出学习的<strong>可置信泛化界</strong>：<br>$R(h) \le R_{emp}(h) + \sqrt{\frac{VC(H)(\ln\frac{2m}{VC(H)}+1)+\ln\frac{4}{\delta}}{m}}$<br>其中：</p>
<ul>
<li>$R(h)$：泛化误差</li>
<li>$R_{emp}(h)$：训练误差</li>
<li>$m$：样本数</li>
<li>$\delta$：置信度</li>
</ul>
<p>可以理解为：<br>$R(h) \le R_{emp}(h) + \underbrace{\sqrt{\frac{\color{blue}{\text{模型复杂度}} + \color{green}{\text{置信项}}}{\color{red}{m}}}}_{间隔项}$<br>结论：</p>
<blockquote>
<p><strong>VC 维越大 → 模型容量越强 → 更容易过拟合</strong><br><strong>VC 维越小 → 模型越简单 → 泛化更好</strong></p>
</blockquote>
<h2 id="PCA-（Principal-Component-Analysis-主成分分析"><a href="#PCA-（Principal-Component-Analysis-主成分分析" class="headerlink" title="PCA （Principal Component Analysis) 主成分分析"></a>PCA （Principal Component Analysis) 主成分分析</h2><p><strong>核心思想</strong></p>
<blockquote>
<p>PCA 是一种无监督降维方法，通过最大化投影方差来找到最重要的方向（主成分）。</p>
</blockquote>
<p>换句话说：</p>
<ul>
<li>找一组方向，使数据投影到这些方向上“最有变化”</li>
<li>保留信息最多</li>
<li>减少维度但不引入标签信息</li>
</ul>
<p><strong>目的</strong></p>
<blockquote>
<p>通过线性变换，将高维数据投影到低维空间，使投影后数据方差最大，从而保留数据最重要的信息。</p>
</blockquote>
<p><strong>数学原理</strong>：</p>
<p><strong>Step 1：对数据中心化</strong><br>$x_i \leftarrow x_i - \bar{x}$</p>
<p><strong>Step 2：计算协方差矩阵</strong><br>$S &#x3D; \frac{1}{m} XX^T$</p>
<p><strong>Step 3：求协方差矩阵的特征值与特征向量</strong><br>$S v_i &#x3D; \lambda_i v_i$</p>
<ul>
<li>特征向量：主成分方向</li>
<li>特征值：方差大小</li>
</ul>
<p><strong>Step 4：按特征值从大到小取前 k 个方向</strong></p>
<p><strong>Step 5：进行投影（降维）</strong><br>$z &#x3D; V_k^T x$</p>
<p><strong>本质</strong></p>
<ul>
<li>找<strong>最大方差方向</strong></li>
<li>等价于<strong>重构误差最小化</strong></li>
<li>属于<strong>无监督学习</strong></li>
<li>只关注数据”分布”，不关注分类<strong>可分性</strong></li>
</ul>
<p><strong>Note：</strong></p>
<p>✔ 不利用分类信息<br>✔ 最大化投影方差<br>✔ 通过特征分解&#x2F;奇异值分解（SVD）实现<br>✔ 结果是线性降维<br>✔ 主成分之间正交</p>
<blockquote>
<p>* </p>
</blockquote>
<h2 id="Q-Learning-1"><a href="#Q-Learning-1" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>EM（期望最大化）算法是统计学习中求 <strong>含隐变量模型参数的极大似然估计（MLE）或最大后验估计（MAP）</strong> 的常用方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    A[初始化 Q(s,a)] --&gt; B[观察初始状态 s]</span><br><span class="line">    B --&gt; C[根据 ε-greedy 选择动作 a]</span><br><span class="line">    C --&gt; D[执行动作 a，得到奖励 r 和下一状态 s&#x27;]</span><br><span class="line">    D --&gt; E[使用 Q 更新公式更新 Q(s,a)]</span><br><span class="line">    E --&gt; F[令 s = s&#x27;]</span><br><span class="line">    F --&gt; G&#123;是否终止?&#125;</span><br><span class="line">    G --&gt;|否| C</span><br><span class="line">    G --&gt;|是| H[进入下一轮 Episode]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="（二）统计学习分类器"><a href="#（二）统计学习分类器" class="headerlink" title="（二）统计学习分类器"></a>（二）统计学习分类器</h1><hr>
<h1 id="（三）线性模型与神经网络"><a href="#（三）线性模型与神经网络" class="headerlink" title="（三）线性模型与神经网络"></a>（三）线性模型与神经网络</h1><h2 id="CNN-（Convolutional-Neural-Networks）卷积神经网络"><a href="#CNN-（Convolutional-Neural-Networks）卷积神经网络" class="headerlink" title="CNN （Convolutional Neural Networks）卷积神经网络"></a>CNN （Convolutional Neural Networks）卷积神经网络</h2><ul>
<li>神经元与感知机</li>
</ul>
<p>神经网络的基本单元是神经元（Neuron），灵感来源于生物神经元。感知机（Perceptron）是最早的人工神经元模型，其计算方式为对输入特征进行加权求和后，再加上一个偏置值（bias），最后通过激活函数输出。其数学表达式可写作：</p>
<p><strong>$y &#x3D; f\left( \sum_{i&#x3D;1}^{n} w_i x_i + b \right)$</strong></p>
<blockquote>
<p>$x_i$ 为输入特征向量</p>
<p>$w_i$ 为权重</p>
<p>$b$  为偏执</p>
<p>$f$ 为激活函数（如$sigmoid$、$ReLU$等）</p>
</blockquote>
<ul>
<li>卷积神经网络的核心思想</li>
</ul>
<p>卷积神经网络通过<strong>局部连接（Local receptive field）</strong> + <strong>权值共享（weight sharing）</strong> + <strong>池化（pooling）</strong> 来处理图像、语音等具有空间结构的信号。</p>
<h3 id="CNN-架构"><a href="#CNN-架构" class="headerlink" title="CNN 架构"></a>CNN 架构</h3><blockquote>
<ul>
<li>输入层（Input Layer）</li>
<li>卷积层（Convolutional Layer）</li>
<li>激活函数（Activation Function）</li>
<li>池化层 （Pooling Layer）</li>
<li>全连接层 （Fully Connected Layer）</li>
<li>输出层（Output Layer）</li>
</ul>
</blockquote>
<h1 id="（四）深度学习"><a href="#（四）深度学习" class="headerlink" title="（四）深度学习"></a>（四）深度学习</h1>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%A4%E5%A4%A7%E7%B1%BB"><span class="toc-number">1.0.1.</span> <span class="toc-text">1. 机器学习的两大类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">2. 基本流程：从数据到模型预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%81%87%E8%AE%BE%E3%80%81%E7%9C%9F%E7%9B%B8%E4%B8%8E%E5%AD%A6%E4%B9%A0%E5%99%A8"><span class="toc-number">1.0.3.</span> <span class="toc-text">3. 假设、真相与学习器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%EF%BC%88Hypothesis-Space%EF%BC%89"><span class="toc-number">1.0.4.</span> <span class="toc-text">4. 假设空间（Hypothesis Space）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B%E5%88%86%E7%B1%BB"><span class="toc-number">1.0.5.</span> <span class="toc-text">5. 学习任务类型分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%B5%8B%E8%AF%95%E6%A0%B7%E6%9C%AC%E4%B8%8E%E6%B3%9B%E5%8C%96%EF%BC%88Generalization%EF%BC%89"><span class="toc-number">1.0.6.</span> <span class="toc-text">6. 测试样本与泛化（Generalization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD%EF%BC%88Inductive-Bias%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">归纳偏好（Inductive Bias）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">按学习方式分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.2.</span> <span class="toc-text">按任务类型分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD"><span class="toc-number">2.</span> <span class="toc-text">专业术语</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87-%E4%BC%BC%E7%84%B6%E6%A6%82%E7%8E%87-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">先验概率 似然概率 后验概率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87-Prior-Probability-P-H"><span class="toc-number">2.0.0.1.1.</span> <span class="toc-text">先验概率 (Prior Probability) - $P(H)$</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BC%BC%E7%84%B6%E6%A6%82%E7%8E%87-Likelihood-Probability-P-E-H"><span class="toc-number">2.0.0.1.2.</span> <span class="toc-text">似然概率 (Likelihood Probability) - $P(E|H)$</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87-Posterior-Probability-P-H-E"><span class="toc-number">2.0.0.1.3.</span> <span class="toc-text">后验概率 (Posterior Probability) - $P(H|E)$</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#x-theta-%E5%92%8C-theta-x"><span class="toc-number">2.0.0.2.</span> <span class="toc-text">$x|\theta$ 和$\theta|x$</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#theta-x-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%88Posterior-Probability%EF%BC%89"><span class="toc-number">2.0.0.2.1.</span> <span class="toc-text">$\theta|x$: 后验概率（Posterior Probability）</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%90%E5%8F%98%E9%87%8F"><span class="toc-number">2.0.0.3.</span> <span class="toc-text">隐变量</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95"><span class="toc-number"></span> <span class="toc-text">（一）机器学习基础算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.</span> <span class="toc-text">贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%EF%BC%88%E5%9F%BA%E7%A1%80%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">贝叶斯公式（基础）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%88%86%E7%B1%BB%E8%A7%84%E5%88%99"><span class="toc-number">1.2.</span> <span class="toc-text">贝叶斯分类器的分类规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F%E5%8C%96%E7%AE%80"><span class="toc-number">1.3.</span> <span class="toc-text">后验概率公式化简</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8-%EF%BC%88Naive-Bayes%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">朴素贝叶斯分类器 （Naive Bayes）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.5.</span> <span class="toc-text">贝叶斯分类器与朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%BE%E4%BE%8B"><span class="toc-number">1.6.</span> <span class="toc-text">计算后验概率举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%87%A0%E4%B8%AA%E5%85%B3%E9%94%AE%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-number">1.7.</span> <span class="toc-text">贝叶斯分类器的几个关键知识点</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%85%AC%E5%BC%8F%EF%BC%9AMAP-Maximum-A-Posteriori-Estimation-%E5%86%B3%E7%AD%96"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">1. 公式：MAP (Maximum A Posteriori Estimation) 决策</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%B1%95%E5%BC%80"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">2. 后验概率展开</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%81%87%E8%AE%BE"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">3. 朴素贝叶斯假设</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%B9%B3%E6%BB%91"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">4. 平滑</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Q-Learning"><span class="toc-number"></span> <span class="toc-text">Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#K-means-%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">K-means 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means-%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.1.</span> <span class="toc-text">K-means 算法步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VC%E7%BB%B4"><span class="toc-number">2.</span> <span class="toc-text">VC维</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#VC-%E7%BB%B4%E7%9A%84%E6%AD%A3%E5%BC%8F%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">VC 维的正式定义：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B%E7%9A%84-VC-%E7%BB%B4"><span class="toc-number">2.0.0.2.</span> <span class="toc-text">常见模型的 VC 维</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#VC-%E7%BB%B4%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.0.0.3.</span> <span class="toc-text">VC 维与泛化误差</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA-%EF%BC%88Principal-Component-Analysis-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">PCA （Principal Component Analysis) 主成分分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning-1"><span class="toc-number">4.</span> <span class="toc-text">Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number"></span> <span class="toc-text">（二）统计学习分类器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number"></span> <span class="toc-text">（三）线性模型与神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-%EF%BC%88Convolutional-Neural-Networks%EF%BC%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">CNN （Convolutional Neural Networks）卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN-%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">CNN 架构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%EF%BC%88%E5%9B%9B%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number"></span> <span class="toc-text">（四）深度学习</span></a>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://minjohnzi.github.io/2025/11/26/machine-learning-note/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&text=machine_learning_note"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&is_video=false&description=machine_learning_note"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=machine_learning_note&body=Check out this article: https://minjohnzi.github.io/2025/11/26/machine-learning-note/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&title=machine_learning_note"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&name=machine_learning_note&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://minjohnzi.github.io/2025/11/26/machine-learning-note/&t=machine_learning_note"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2021-2025
    mjmj
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
